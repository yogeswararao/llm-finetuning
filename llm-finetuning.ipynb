{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ea6db5",
   "metadata": {},
   "source": [
    "# LLM Fine-tuning Methods Comparison\n",
    "\n",
    "This notebook demonstrates various parameter-efficient fine-tuning (PEFT) methods for large language models.\n",
    "\n",
    "**Note:** All implementations are abstracted with a shared `CoreFineTuner` class that handles common training, validation, and evaluation logic. Each method only implements its specific configuration and setup.\n",
    "\n",
    "**Methods covered:**\n",
    "1. **Full Fine-tuning** - Updates all parameters (baseline)\n",
    "2. **LoRA** - Low-rank adaptation with trainable A and B matrices\n",
    "3. **LoRA-FA** - LoRA with frozen A matrix (only B trainable)\n",
    "4. **LoRA+** - LoRA with different learning rates for A and B\n",
    "5. **Delta-LoRA** - LoRA with base weight updates via approximation\n",
    "6. **AdaLoRA** - Adaptive LoRA with dynamic rank allocation\n",
    "7. **QLoRA** - Quantized LoRA with 4-bit base weights\n",
    "8. **VeRA** - Vector-based random matrix adaptation\n",
    "9. **Prompt Tuning** - Simple learnable prompt tokens\n",
    "10. **P-Tuning** - Prompt tuning with encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba47a6ea-ec14-47a2-81cd-1a9ab6fb4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetuner import (\n",
    "    FullFineTuner,\n",
    "    LoRAFineTuner,\n",
    "    LoRAFAFineTuner,\n",
    "    LoRAPlusFineTuner,\n",
    "    DeltaLoRAFineTuner,\n",
    "    AdaLoRAFineTuner,\n",
    "    QLoRAFineTuner,\n",
    "    VeRAFineTuner,\n",
    "    PromptTuningFineTuner,\n",
    "    PTuningFineTuner\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51c984-34d0-4307-a689-cbf5c05f15b6",
   "metadata": {},
   "source": [
    "## Full Fine-tuning\n",
    "\n",
    "Updates all model parameters during training.\n",
    "\n",
    "- Achieves high accuracy. But requires most memory\n",
    "- All weights are trainable\n",
    "- Best for when you have sufficient computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190fc1d4-8d4b-4c19-8e4b-b94d010eb1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Full Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [04:36<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3088\n",
      "Epoch 1 - Validation Loss: 0.1774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [04:37<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.1617\n",
      "Epoch 2 - Validation Loss: 0.2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [04:37<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.0895\n",
      "Epoch 3 - Validation Loss: 0.3429\n",
      "Model saved to models/full_finetuner\n",
      "\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:37<00:00,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2871\n",
      "Accuracy: 0.9332 (93.32%)\n",
      "Precision: 0.9333\n",
      "Recall: 0.9332\n",
      "F1 Score: 0.9332\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_finetuner = FullFineTuner()\n",
    "full_finetuner.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fb6f7-7d6a-4305-bda4-48d1de17773d",
   "metadata": {},
   "source": [
    "## LoRA (Low-Rank Adaptation)\n",
    "\n",
    "Adds trainable low-rank matrices (A and B) to specific layers while keeping base weights frozen.\n",
    "\n",
    "- Weight update: W' = W + BA\n",
    "- Only ~1-2% of parameters are trainable\n",
    "- Memory efficient with competitive performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ae5a62-848e-477f-90d8-53ecd5f583fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 67,842,052 || trainable%: 1.3075\n",
      "============================================================\n",
      "LoRA Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [03:52<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.2986\n",
      "Epoch 1 - Validation Loss: 0.2036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [03:52<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2103\n",
      "Epoch 2 - Validation Loss: 0.1836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:50<00:00,  7.05it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.1996\n",
      "Accuracy: 0.9262 (92.62%)\n",
      "Precision: 0.9263\n",
      "Recall: 0.9262\n",
      "F1 Score: 0.9262\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora = LoRAFineTuner()\n",
    "lora.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e57331-2762-4a2c-acbb-2ecc5e5c20e0",
   "metadata": {},
   "source": [
    "## LoRA-FA (LoRA with Frozen-A)\n",
    "\n",
    "Only trains the B matrix while keeping A frozen, reducing trainable parameters by 50% compared to standard LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d02d09-a697-4b4d-970c-4e3726ebda76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 739,586 || all params: 67,842,052 || trainable%: 1.0902\n",
      "============================================================\n",
      "LoRA-FA Fine-tuning\n",
      "============================================================\n",
      "Key Feature: Only matrix B is trained, matrix A is frozen\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting LoRA-FA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [03:47<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3292\n",
      "Epoch 1 - Validation Loss: 0.2582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [03:47<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2412\n",
      "Epoch 2 - Validation Loss: 0.2750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [03:47<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.2206\n",
      "Epoch 3 - Validation Loss: 0.2216\n",
      "Model saved to models/lora_fa\n",
      "\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:50<00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2178\n",
      "Accuracy: 0.9120 (91.20%)\n",
      "Precision: 0.9121\n",
      "Recall: 0.9120\n",
      "F1 Score: 0.9120\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora_fa = LoRAFAFineTuner()\n",
    "lora_fa.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe961e50-9afa-4646-9cfd-95f6ec300712",
   "metadata": {},
   "source": [
    "## LoRA+\n",
    "\n",
    "- Different learning rates for A and B matrices\n",
    "- B matrix uses much higher learning rate (16-32x)\n",
    "- Improved training stability and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228c70d4-a9dc-4741-bcfe-a5ac5244baf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 67,842,052 || trainable%: 1.3075\n",
      "============================================================\n",
      "LoRA+ Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting LoRA+ training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [03:52<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3006\n",
      "Epoch 1 - Validation Loss: 0.2805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [03:53<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2118\n",
      "Epoch 2 - Validation Loss: 0.2317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [03:53<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.1647\n",
      "Epoch 3 - Validation Loss: 0.2372\n",
      "Model saved to models/lora_plus\n",
      "\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:51<00:00,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2103\n",
      "Accuracy: 0.9280 (92.80%)\n",
      "Precision: 0.9281\n",
      "Recall: 0.9280\n",
      "F1 Score: 0.9280\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora_plus = LoRAPlusFineTuner()\n",
    "lora_plus.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b18d4d-f6bd-499e-b573-545f9d89a745",
   "metadata": {},
   "source": [
    "## Delta-LoRA - Approximation\n",
    "\n",
    "Updates both LoRA adapters and base weights. The difference between the product of low-rank matrices A and B in two consecutive steps is added to W. This implementation used an approximation method with different learning rate for base weights instead of directly computing the delta.\n",
    "\n",
    "- Combines LoRA efficiency with direct base weight updates\n",
    "- Base weights are trainable\n",
    "- Approximates delta mechanism using a smaller learning rate for base weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c765cd-a474-479b-ae1b-5f750a4972a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [05:05<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3268\n",
      "Epoch 1 - Validation Loss: 0.2114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [05:05<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2174\n",
      "Epoch 2 - Validation Loss: 0.2245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [05:05<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.1876\n",
      "Epoch 3 - Validation Loss: 0.1865\n",
      "Model saved to models/delta_lora\n",
      "\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:50<00:00,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2194\n",
      "Accuracy: 0.9231 (92.31%)\n",
      "Precision: 0.9235\n",
      "Recall: 0.9231\n",
      "F1 Score: 0.9231\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "delta_lora = DeltaLoRAFineTuner()\n",
    "delta_lora.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc88176-b230-47f5-8001-4a04a0b044a5",
   "metadata": {},
   "source": [
    "## AdaLoRA (Adaptive LoRA)\n",
    "\n",
    "Dynamically adjusts the rank of LoRA adapters during training.\n",
    "\n",
    "- Adaptive rank allocation based on importance\n",
    "- Prunes less important adapters during training\n",
    "- More efficient parameter usage than standard LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1168a9f0-0ce1-4152-98de-a257c20bfc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,034,786 || all params: 67,989,820 || trainable%: 1.5220\n",
      "============================================================\n",
      "AdaLoRA Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting AdaLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [04:08<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.5214\n",
      "Epoch 1 - Validation Loss: 0.2677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [04:08<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2529\n",
      "Epoch 2 - Validation Loss: 0.2335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:55<00:00,  6.75it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2211\n",
      "Accuracy: 0.9110 (91.10%)\n",
      "Precision: 0.9110\n",
      "Recall: 0.9110\n",
      "F1 Score: 0.9110\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ada_lora = AdaLoRAFineTuner()\n",
    "ada_lora.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc34cd6-4856-4949-a9b2-0402b2223904",
   "metadata": {},
   "source": [
    "## QLoRA (Quantized LoRA)\n",
    "\n",
    "Combines 4-bit quantization with LoRA for memory-efficient fine-tuning of large models.\n",
    "\n",
    "- Base model weights quantized to 4-bit (NF4)\n",
    "- LoRA adapters remain in full precision\n",
    "- Significantly reduces memory requirements\n",
    "\n",
    "**Note:** Requires x86_64 Linux with CUDA (NVIDIA GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "180922a5-2726-47bc-a670-12b08609a942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296,450 || all params: 67,249,922 || trainable%: 0.4408\n",
      "============================================================\n",
      "QLoRA Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [04:18<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3266\n",
      "Epoch 1 - Validation Loss: 0.2134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [04:18<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2159\n",
      "Epoch 2 - Validation Loss: 0.2045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [04:18<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.1861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yogi/my-repos/llm-finetuning/.venv/lib/python3.13/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in /tmp/qlora_checkpoint_nmkxnk2u - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation Loss: 0.1871\n",
      "Model saved to models/qlora\n",
      "\n",
      "Evaluating after fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:28<00:00,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2036\n",
      "Accuracy: 0.9256 (92.56%)\n",
      "Precision: 0.9258\n",
      "Recall: 0.9256\n",
      "F1 Score: 0.9256\n",
      "============================================================\n",
      "\n",
      "\n",
      "QLoRA fine-tuning completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qlora = QLoRAFineTuner()\n",
    "qlora.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b58593-be46-4dcf-9972-584a5cbff943",
   "metadata": {},
   "source": [
    "## VeRA (Vector-based Random Matrix Adaptation)\n",
    "\n",
    "Uses shared frozen random matrices (A and B) with trainable scaling vectors (b and d).\n",
    "\n",
    "- Only two trainable vectors per module (b and d)\n",
    "- A and B matrices are frozen shared random matrices\n",
    "- Extremely parameter efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed576e6b-6fe0-4482-bb01-09eb3ae20676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 616,706 || all params: 67,571,716 || trainable%: 0.9127\n",
      "============================================================\n",
      "VeRA Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting VeRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [04:14<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3515\n",
      "Epoch 1 - Validation Loss: 0.3329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [04:15<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.2315\n",
      "Epoch 3 - Validation Loss: 0.2231\n",
      "Model saved to models/vera\n",
      "\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [02:01<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2224\n",
      "Accuracy: 0.9102 (91.02%)\n",
      "Precision: 0.9104\n",
      "Recall: 0.9102\n",
      "F1 Score: 0.9102\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vera = VeRAFineTuner()\n",
    "vera.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87499923-78ee-4de3-a81c-66b52406c110",
   "metadata": {},
   "source": [
    "## Prompt Tuning\n",
    "\n",
    "Adds learnable prompt tokens to input sequences that guide model behavior without modifying base weights.\n",
    "\n",
    "- Base model weights remain completely frozen\n",
    "- Learnable prompt tokens prepended to inputs\n",
    "- Highly parameter-efficient (only prompt embeddings are trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f03ef30d-aa86-4a48-9e67-bffbd4ee8109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 607,490 || all params: 67,562,500 || trainable%: 0.8992\n",
      "============================================================\n",
      "Prompt Tuning Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting Prompt Tuning training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [03:27<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.5345\n",
      "Epoch 1 - Validation Loss: 0.4441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [03:28<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.4078\n",
      "Epoch 2 - Validation Loss: 0.3985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [03:28<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.3760\n",
      "Epoch 3 - Validation Loss: 0.3791\n",
      "Model saved to models/prompt_tuning\n",
      "\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:38<00:00,  7.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.3425\n",
      "Accuracy: 0.8522 (85.22%)\n",
      "Precision: 0.8523\n",
      "Recall: 0.8522\n",
      "F1 Score: 0.8522\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_tuning = PromptTuningFineTuner()\n",
    "prompt_tuning.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b9f51-34c1-4d70-950a-2c6f742423cf",
   "metadata": {},
   "source": [
    "## P-Tuning\n",
    "\n",
    "Uses a prompt encoder (MLP/LSTM) to generate prompt representations that are optimized across transformer layers.\n",
    "\n",
    "- Base model weights remain frozen\n",
    "- Prompt encoder (MLP/LSTM) generates prompt representations\n",
    "- More complex than simple prompt tuning, typically better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb962307-a886-46d7-8dba-faf026536361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,379,266 || all params: 69,334,276 || trainable%: 3.4316\n",
      "============================================================\n",
      "P-Tuning Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting P-Tuning training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [03:28<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3692\n",
      "Epoch 1 - Validation Loss: 0.2619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [03:28<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2818\n",
      "Epoch 2 - Validation Loss: 0.2596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [03:28<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.2606\n",
      "Epoch 3 - Validation Loss: 0.2447\n",
      "Model saved to models/p_tuning\n",
      "\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:38<00:00,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2440\n",
      "Accuracy: 0.8996 (89.96%)\n",
      "Precision: 0.8998\n",
      "Recall: 0.8996\n",
      "F1 Score: 0.8995\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p_tuning = PTuningFineTuner()\n",
    "p_tuning.run(save_model=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-finetuning",
   "language": "python",
   "name": "llm-finetuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
