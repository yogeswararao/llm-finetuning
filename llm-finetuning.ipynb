{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ea6db5",
   "metadata": {},
   "source": [
    "# LLM Fine-tuning Methods Comparison\n",
    "\n",
    "This notebook demonstrates various parameter-efficient fine-tuning (PEFT) methods for large language models.\n",
    "\n",
    "**Note:** All implementations are abstracted with a shared `BaseFineTuner` class that handles common training, validation, and evaluation logic. Each method only implements its specific configuration and setup, making the codebase clean and maintainable.\n",
    "\n",
    "**Methods covered:**\n",
    "1. **Full Fine-tuning** - Updates all parameters (baseline)\n",
    "2. **LoRA** - Low-rank adaptation with trainable A and B matrices\n",
    "3. **LoRA-FA** - LoRA with frozen A matrix (only B trainable)\n",
    "4. **LoRA+** - LoRA with different learning rates for A and B\n",
    "5. **Delta-LoRA** - LoRA with base weight updates via approximation\n",
    "6. **AdaLoRA** - Adaptive LoRA with dynamic rank allocation\n",
    "7. **QLoRA** - Quantized LoRA with 4-bit base weights\n",
    "8. **VeRA** - Vector-based random matrix adaptation\n",
    "9. **Prompt Tuning** - Simple learnable prompt tokens\n",
    "10. **P-Tuning** - Prompt tuning with encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47a6ea-ec14-47a2-81cd-1a9ab6fb4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.methods import (\n",
    "    FullFineTuner,\n",
    "    LoRAFineTuner,\n",
    "    LoRAFAFineTuner,\n",
    "    LoRAPlusFineTuner,\n",
    "    DeltaLoRAFineTuner,\n",
    "    AdaLoRAFineTuner,\n",
    "    QLoRAFineTuner,\n",
    "    VeRAFineTuner,\n",
    "    PromptTuningFineTuner,\n",
    "    PTuningFineTuner\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51c984-34d0-4307-a689-cbf5c05f15b6",
   "metadata": {},
   "source": [
    "## Full Fine-tuning\n",
    "\n",
    "Updates all model parameters during training.\n",
    "\n",
    "- Achieves high accuracy. But requires most memory\n",
    "- All weights are trainable\n",
    "- Best for when you have sufficient computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190fc1d4-8d4b-4c19-8e4b-b94d010eb1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Full Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [04:38<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3033\n",
      "Epoch 1 - Validation Loss: 0.1808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [04:38<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.1628\n",
      "Epoch 2 - Validation Loss: 0.1732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [04:38<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.0904\n",
      "Epoch 3 - Validation Loss: 0.2632\n",
      "Model saved to models/full_finetuning\n",
      "\n",
      "Evaluating after fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:38<00:00,  7.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2835\n",
      "Accuracy: 0.9326 (93.26%)\n",
      "Precision: 0.9326\n",
      "Recall: 0.9326\n",
      "F1 Score: 0.9326\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11629   871]\n",
      " [  815 11685]]\n",
      "============================================================\n",
      "\n",
      "\n",
      "Full fine-tuning completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_finetuner = FullFineTuner()\n",
    "full_finetuner.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fb6f7-7d6a-4305-bda4-48d1de17773d",
   "metadata": {},
   "source": [
    "## LoRA (Low-Rank Adaptation)\n",
    "\n",
    "Adds trainable low-rank matrices (A and B) to specific layers while keeping base weights frozen.\n",
    "\n",
    "- Weight update: W' = W + BA\n",
    "- Only ~1-2% of parameters are trainable\n",
    "- Memory efficient with competitive performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ae5a62-848e-477f-90d8-53ecd5f583fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 67,842,052 || trainable%: 1.3075\n",
      "============================================================\n",
      "LoRA Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [03:52<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3041\n",
      "Epoch 1 - Validation Loss: 0.2450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [03:53<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2089\n",
      "Epoch 2 - Validation Loss: 0.1989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [03:53<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.1775\n",
      "Epoch 3 - Validation Loss: 0.1899\n",
      "Model saved to models/lora\n",
      "\n",
      "Evaluating after fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:51<00:00,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2054\n",
      "Accuracy: 0.9250 (92.50%)\n",
      "Precision: 0.9254\n",
      "Recall: 0.9250\n",
      "F1 Score: 0.9249\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11371  1129]\n",
      " [  747 11753]]\n",
      "============================================================\n",
      "\n",
      "\n",
      "LoRA fine-tuning completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora = LoRAFineTuner()\n",
    "lora.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e57331-2762-4a2c-acbb-2ecc5e5c20e0",
   "metadata": {},
   "source": [
    "## LoRA-FA (LoRA with Frozen-A)\n",
    "\n",
    "Only trains the B matrix while keeping A frozen, reducing trainable parameters by 50% compared to standard LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d02d09-a697-4b4d-970c-4e3726ebda76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 739,586 || all params: 67,842,052 || trainable%: 1.0902\n",
      "============================================================\n",
      "LoRA-FA Fine-tuning\n",
      "============================================================\n",
      "Key Feature: Only matrix B is trained, matrix A is frozen\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting LoRA-FA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [03:47<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3308\n",
      "Epoch 1 - Validation Loss: 0.2449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [03:48<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2397\n",
      "Epoch 2 - Validation Loss: 0.2154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [03:48<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.2189\n",
      "Epoch 3 - Validation Loss: 0.2051\n",
      "Model saved to models/lora_fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:51<00:00,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2165\n",
      "Accuracy: 0.9148 (91.48%)\n",
      "Precision: 0.9148\n",
      "Recall: 0.9148\n",
      "F1 Score: 0.9148\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11416  1084]\n",
      " [ 1045 11455]]\n",
      "============================================================\n",
      "\n",
      "\n",
      "LoRA-FA fine-tuning completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora_fa = LoRAFAFineTuner()\n",
    "lora_fa.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe961e50-9afa-4646-9cfd-95f6ec300712",
   "metadata": {},
   "source": [
    "## LoRA+\n",
    "\n",
    "- Different learning rates for A and B matrices\n",
    "- B matrix uses much higher learning rate (16-32x)\n",
    "- Improved training stability and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "228c70d4-a9dc-4741-bcfe-a5ac5244baf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 67,842,052 || trainable%: 1.3075\n",
      "============================================================\n",
      "LoRA+ Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting LoRA+ training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [03:52<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.2974\n",
      "Epoch 1 - Validation Loss: 0.2276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [03:53<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2076\n",
      "Epoch 2 - Validation Loss: 0.1862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [03:53<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.1629\n",
      "Epoch 3 - Validation Loss: 0.1985\n",
      "Model saved to models/lora_plus\n",
      "\n",
      "Evaluating after fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:51<00:00,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2184\n",
      "Accuracy: 0.9284 (92.84%)\n",
      "Precision: 0.9286\n",
      "Recall: 0.9284\n",
      "F1 Score: 0.9284\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11497  1003]\n",
      " [  786 11714]]\n",
      "============================================================\n",
      "\n",
      "\n",
      "LoRA+ fine-tuning completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora_plus = LoRAPlusFineTuner()\n",
    "lora_plus.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b18d4d-f6bd-499e-b573-545f9d89a745",
   "metadata": {},
   "source": [
    "## Delta-LoRA - Approximation\n",
    "\n",
    "Updates both LoRA adapters and base weights. The difference between the product of low-rank matrices A and B in two consecutive steps is added to W. This implementation used an approximation method with different learning rate for base weights instead of directly computing the delta.\n",
    "\n",
    "- Combines LoRA efficiency with direct base weight updates\n",
    "- Base weights are trainable\n",
    "- Approximates delta mechanism using a smaller learning rate for base weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c765cd-a474-479b-ae1b-5f750a4972a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 67,842,052 || all params: 67,842,052 || trainable%: 100.0000\n",
      "============================================================\n",
      "Delta-LoRA Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting Delta-LoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [05:05<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3380\n",
      "Epoch 1 - Validation Loss: 0.2161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [05:06<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2176\n",
      "Epoch 2 - Validation Loss: 0.1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [05:06<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.1877\n",
      "Epoch 3 - Validation Loss: 0.1857\n",
      "Model saved to models/delta_lora\n",
      "\n",
      "Evaluating after fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:51<00:00,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2185\n",
      "Accuracy: 0.9255 (92.55%)\n",
      "Precision: 0.9257\n",
      "Recall: 0.9255\n",
      "F1 Score: 0.9255\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11441  1059]\n",
      " [  804 11696]]\n",
      "============================================================\n",
      "\n",
      "\n",
      "Delta-LoRA fine-tuning completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "delta_lora = DeltaLoRAFineTuner()\n",
    "delta_lora.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc88176-b230-47f5-8001-4a04a0b044a5",
   "metadata": {},
   "source": [
    "## AdaLoRA (Adaptive LoRA)\n",
    "\n",
    "Dynamically adjusts the rank of LoRA adapters during training.\n",
    "\n",
    "- Adaptive rank allocation based on importance\n",
    "- Prunes less important adapters during training\n",
    "- More efficient parameter usage than standard LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1168a9f0-0ce1-4152-98de-a257c20bfc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,034,786 || all params: 67,989,820 || trainable%: 1.5220\n",
      "============================================================\n",
      "AdaLoRA Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting AdaLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [04:08<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.5267\n",
      "Epoch 1 - Validation Loss: 0.3245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [04:09<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2559\n",
      "Epoch 2 - Validation Loss: 0.2293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [04:09<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.2365\n",
      "Epoch 3 - Validation Loss: 0.2197\n",
      "Model saved to models/adalora\n",
      "\n",
      "Evaluating after fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:56<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2221\n",
      "Accuracy: 0.9108 (91.08%)\n",
      "Precision: 0.9109\n",
      "Recall: 0.9108\n",
      "F1 Score: 0.9108\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11292  1208]\n",
      " [ 1021 11479]]\n",
      "============================================================\n",
      "\n",
      "\n",
      "AdaLoRA fine-tuning completed!\n"
     ]
    }
   ],
   "source": [
    "ada_lora = AdaLoRAFineTuner()\n",
    "ada_lora.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc34cd6-4856-4949-a9b2-0402b2223904",
   "metadata": {},
   "source": [
    "## QLoRA (Quantized LoRA)\n",
    "\n",
    "Combines 4-bit quantization with LoRA for memory-efficient fine-tuning of large models.\n",
    "\n",
    "- Base model weights quantized to 4-bit (NF4)\n",
    "- LoRA adapters remain in full precision\n",
    "- Significantly reduces memory requirements\n",
    "\n",
    "**Note:** Requires x86_64 Linux with CUDA (NVIDIA GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "180922a5-2726-47bc-a670-12b08609a942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296,450 || all params: 67,249,922 || trainable%: 0.4408\n",
      "============================================================\n",
      "QLoRA Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting QLoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [04:18<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3331\n",
      "Epoch 1 - Validation Loss: 0.2037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [04:18<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2116\n",
      "Epoch 2 - Validation Loss: 0.1642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [04:18<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.1857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yogi/my-repos/llm-finetuning/.venv/lib/python3.13/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in /tmp/qlora_checkpoint_pshw12hb - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation Loss: 0.1833\n",
      "Model saved to models/qlora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:28<00:00,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2028\n",
      "Accuracy: 0.9258 (92.58%)\n",
      "Precision: 0.9260\n",
      "Recall: 0.9258\n",
      "F1 Score: 0.9258\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11468  1032]\n",
      " [  822 11678]]\n",
      "============================================================\n",
      "\n",
      "\n",
      "QLoRA fine-tuning completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qlora = QLoRAFineTuner()\n",
    "qlora.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b58593-be46-4dcf-9972-584a5cbff943",
   "metadata": {},
   "source": [
    "## VeRA (Vector-based Random Matrix Adaptation)\n",
    "\n",
    "Uses shared frozen random matrices (A and B) with trainable scaling vectors (b and d).\n",
    "\n",
    "- Only two trainable vectors per module (b and d)\n",
    "- A and B matrices are frozen shared random matrices\n",
    "- Extremely parameter efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed576e6b-6fe0-4482-bb01-09eb3ae20676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 616,706 || all params: 67,571,716 || trainable%: 0.9127\n",
      "============================================================\n",
      "VeRA Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting VeRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [04:16<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3521\n",
      "Epoch 1 - Validation Loss: 0.2781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [04:16<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.2537\n",
      "Epoch 2 - Validation Loss: 0.2246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [04:16<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.2349\n",
      "Epoch 3 - Validation Loss: 0.2150\n",
      "Model saved to models/vera\n",
      "\n",
      "Evaluating after fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [02:02<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2230\n",
      "Accuracy: 0.9102 (91.02%)\n",
      "Precision: 0.9104\n",
      "Recall: 0.9102\n",
      "F1 Score: 0.9102\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11227  1273]\n",
      " [  972 11528]]\n",
      "============================================================\n",
      "\n",
      "\n",
      "VeRA fine-tuning completed!\n"
     ]
    }
   ],
   "source": [
    "vera = VeRAFineTuner()\n",
    "vera.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87499923-78ee-4de3-a81c-66b52406c110",
   "metadata": {},
   "source": [
    "## Prompt Tuning\n",
    "\n",
    "Adds learnable prompt tokens to input sequences that guide model behavior without modifying base weights.\n",
    "\n",
    "- Base model weights remain completely frozen\n",
    "- Learnable prompt tokens prepended to inputs\n",
    "- Highly parameter-efficient (only prompt embeddings are trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f03ef30d-aa86-4a48-9e67-bffbd4ee8109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 607,490 || all params: 67,562,500 || trainable%: 0.8992\n",
      "============================================================\n",
      "Prompt Tuning Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting Prompt Tuning training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [03:27<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.5103\n",
      "Epoch 1 - Validation Loss: 0.4181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [03:29<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.3846\n",
      "Epoch 2 - Validation Loss: 0.3761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [03:29<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.3613\n",
      "Epoch 3 - Validation Loss: 0.3655\n",
      "Model saved to models/prompt_tuning\n",
      "\n",
      "Evaluating after fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:38<00:00,  7.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.3316\n",
      "Accuracy: 0.8588 (85.88%)\n",
      "Precision: 0.8591\n",
      "Recall: 0.8588\n",
      "F1 Score: 0.8588\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10552  1948]\n",
      " [ 1581 10919]]\n",
      "============================================================\n",
      "\n",
      "\n",
      "Prompt Tuning fine-tuning completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_tuning = PromptTuningFineTuner()\n",
    "prompt_tuning.run(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b9f51-34c1-4d70-950a-2c6f742423cf",
   "metadata": {},
   "source": [
    "## P-Tuning\n",
    "\n",
    "Uses a prompt encoder (MLP/LSTM) to generate prompt representations that are optimized across transformer layers.\n",
    "\n",
    "- Base model weights remain frozen\n",
    "- Prompt encoder (MLP/LSTM) generates prompt representations\n",
    "- More complex than simple prompt tuning, typically better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb962307-a886-46d7-8dba-faf026536361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,379,266 || all params: 69,334,276 || trainable%: 3.4316\n",
      "============================================================\n",
      "P-Tuning v2 Fine-tuning\n",
      "============================================================\n",
      "\n",
      "Loading IMDB dataset...\n",
      "\n",
      "Starting P-Tuning v2 training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1563/1563 [03:28<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.3967\n",
      "Epoch 1 - Validation Loss: 0.3889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [03:29<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.3105\n",
      "Epoch 2 - Validation Loss: 0.2878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [03:29<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.2842\n",
      "Epoch 3 - Validation Loss: 0.2849\n",
      "Model saved to models/p_tuning_v2\n",
      "\n",
      "Evaluating after fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:38<00:00,  7.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Loss: 0.2702\n",
      "Accuracy: 0.8894 (88.94%)\n",
      "Precision: 0.8895\n",
      "Recall: 0.8894\n",
      "F1 Score: 0.8894\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11036  1464]\n",
      " [ 1301 11199]]\n",
      "============================================================\n",
      "\n",
      "\n",
      "P-Tuning v2 fine-tuning completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p_tuning = PTuningFineTuner()\n",
    "p_tuning.run(save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bfbe77-dfd7-4ce5-905d-d090f400bb11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-finetuning",
   "language": "python",
   "name": "llm-finetuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
